Project: Convo-AI - Local AI Assistant

Goal: Build a locally-hosted AI assistant with natural conversation capabilities, optimized for Mac M-series chips.

YOUR INSTRUCTIONS:
1. Update project context after milestones.
2. This project is ran with minimal human oversight. Make sure to refresh your view of the wider picture often.
3. Do everything in local environments where possible 
 
Core Components:
1. LLM Integration (Ollama)
   - Support for multiple models (Mistral, Phi-3)
   - Streaming responses
   - Error handling and retries
   - Model switching capability

2. Text-to-Speech (TTS)
   - Sesame CSM-1B integration
   - MPS (Metal Performance Shaders) optimization
   - Audio file management
   - Resource cleanup

3. Mac Optimizations
   - MPS device support
   - GPU layer configuration
   - Batch size optimization
   - Memory management

4. Testing & Quality
   - Comprehensive test suite ✓
   - CI integration with GitHub Actions ✓
   - Coverage reporting and badge integration ✓
   - Error handling validation ✓
   - Mock LLM service in CI environment ✓

5. Project Management
   - Progress tracking
   - Performance metrics
   - Demo scripts
   - Documentation

Development Guidelines:
1. Code Style
   - Use type hints
   - Write short, focused functions
   - Include docstrings
   - Follow functional programming principles

2. Testing
   - Write tests for all new features ✓
   - Maintain high coverage (target 70%+) ✓
   - Include edge cases ✓
   - Mock services in CI environment ✓

3. Performance
   - Monitor memory usage
   - Track response times
   - Optimize for M-series chips
   - Profile critical paths

4. Documentation
   - Keep README updated
   - Document API changes
   - Include usage examples
   - Maintain changelog

Current Status:
- LLM client with streaming support ✓
- TTS client with MPS optimization ✓
- Comprehensive test suite ✓
- Configuration system ✓
- CI integration with GitHub Actions ✓
- Test coverage reporting (89%) ✓
- Mock LLM service in CI ✓

Next Steps:
1. Add performance benchmarks
2. Create demo scripts
3. Enhance documentation
4. Implement caching for LLM responses to improve performance
5. Add support for audio input (speech-to-text)