Project: Convo-AI - Local AI Assistant

Goal: Build a locally-hosted AI assistant with natural conversation capabilities, optimized for Mac M-series chips.

YOUR INSTRUCTIONS:
1. Update project context after milestones.
2. This project is ran with minimal human oversight. Make sure to refresh your view of the wider picture often.
3. Do everything in local environments where possible.
4. You should regularly run tests, check coverage, write new tests, fix things identified by failing tests.
5. Regularly update the GitHub repo.
6. IMPORTANT: Always check and update reports in reports/ directory:
   - Update test_plan.md when modifying test coverage
   - Update progress_report.md weekly with new metrics
   - Use reports/demos/ for example code
   - Keep metrics up to date using @benchmark decorator
 
Core Components:
1. LLM Integration (Ollama)
   - Support for multiple models (Mistral, Phi-3)
   - Streaming responses
   - Error handling and retries
   - Model switching capability

2. Text-to-Speech (TTS)
   - Sesame CSM-1B integration
   - MPS (Metal Performance Shaders) optimization
   - Audio file management
   - Resource cleanup

3. Mac Optimizations
   - MPS device support
   - GPU layer configuration
   - Batch size optimization
   - Memory management

4. Testing & Quality
   - Comprehensive test suite ✓
   - CI integration with GitHub Actions ✓
   - Coverage reporting and badge integration ✓
   - Error handling validation ✓
   - Mock LLM service in CI environment ✓
   - Performance benchmarking system ✓

5. Project Management
   - Progress tracking via reports/ ✓
   - Performance metrics via benchmarks.py ✓
   - Demo scripts ✓
   - Documentation ✓

Development Guidelines:
1. Code Style
   - Use type hints
   - Write short, focused functions
   - Include docstrings
   - Follow functional programming principles

2. Testing
   - Write tests for all new features ✓
   - Maintain high coverage (target 90%+) ✓
   - Include edge cases ✓
   - Mock services in CI environment ✓
   - Track test progress in test_plan.md

3. Performance
   - Monitor memory usage via @benchmark
   - Track response times via @benchmark
   - Optimize for M-series chips
   - Profile critical paths
   - Record metrics in progress_report.md

4. Documentation
   - Keep README updated
   - Document API changes
   - Include usage examples
   - Maintain changelog
   - Update reports/ regularly

Current Status:
- LLM client with streaming support ✓
- TTS client with MPS optimization ✓
- Comprehensive test suite ✓
- Configuration system ✓
- CI integration with GitHub Actions ✓
- Test coverage reporting (92%) ✓
- Mock LLM service in CI ✓
- Performance benchmarks ✓
- Demo scripts ✓
- Progress tracking system ✓

Next Steps:
1. Improve Native TTS module test coverage
2. Add stress testing capabilities
3. Implement automated performance regression detection
4. Add more cache module edge case tests